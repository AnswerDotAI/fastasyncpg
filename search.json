[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "fastasyncpg",
    "section": "",
    "text": "fastasyncpg is a simple wrapper for asyncpg. We’ll explain how it works and build up the module in a “literate” nbdev style.",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#installation",
    "href": "core.html#installation",
    "title": "fastasyncpg",
    "section": "Installation",
    "text": "Installation\nOn macOS, the recommended way to install PostgreSQL is via Homebrew. Other options include Postgres.app (a menubar app) and the EDB installer, but Homebrew integrates best with command-line workflows and makes updates simple.\nTo install PostgreSQL 18 (the current latest stable release):\nbrew install postgresql@18\nTo check if you already have PostgreSQL installed via Homebrew, run brew list | grep postgres. You can also check which version is in your PATH with psql --version.\nLet’s verify the installation is working:\n\n!brew list | grep postgres\n\npostgresql@18\n\n\n\n!psql --version\n\npsql (PostgreSQL) 18.1 (Homebrew)\n\n\nAfter installation, run brew info postgresql@18 to see setup instructions. PostgreSQL 18 is “keg-only”, meaning it’s not automatically symlinked into your PATH.\nYou’ll see something like:\nThis formula has created a default database cluster with:\n  initdb --locale=en_US.UTF-8 -E UTF-8 /opt/homebrew/var/postgresql@18\n\nWhen uninstalling, some dead symlinks are left behind so you may want to run:\n  brew cleanup --prune-prefix\n\npostgresql@18 is keg-only, which means it was not symlinked into /opt/homebrew,\nbecause this is an alternate version of another formula.\n\nIf you need to have postgresql@18 first in your PATH, run:\n  echo 'export PATH=\"/opt/homebrew/opt/postgresql@18/bin:$PATH\"' &gt;&gt; /Users/jhoward/.bash_profile\n\nTo start postgresql@18 now and restart at login:\n  brew services start postgresql@18\nThe brew info output (above) tells you exactly what to do:\n\nAdd to PATH (for bash): echo 'export PATH=\"/opt/homebrew/opt/postgresql@18/bin:$PATH\"' &gt;&gt; ~/.bash_profile && source ~/.bash_profile\nStart the service: brew services start postgresql@18\n\nThis registers PostgreSQL to start automatically at login.\nTo run non-interactive queries from a shell, use -c to pass a command directly:\n\n!psql -d postgres -c \"SELECT version();\"\n\n                                                           version                                                           \n-----------------------------------------------------------------------------------------------------------------------------\n PostgreSQL 18.1 (Homebrew) on aarch64-apple-darwin25.2.0, compiled by Apple clang version 17.0.0 (clang-1700.6.3.2), 64-bit\n(1 row)\n\n\n\nRunning brew services start registers PostgreSQL to start automatically at login/reboot. You can verify this with brew services list, which shows all Homebrew-managed services and their status.\n\n!brew services list\n\nName          Status  User    File\ncloudflared   none            \npostgresql@18 started jhoward ~/Library/LaunchAgents/homebrew.mxcl.postgresql@18.plist\nunbound       none            \n\n\nTo control auto-start behavior: - brew services stop postgresql@18 — stop and disable auto-start - brew services start postgresql@18 — start and enable auto-start\n- brew services run postgresql@18 — run once without enabling auto-start\nOn Ubuntu, the standard way to get the latest PostgreSQL is through the official PostgreSQL APT repository (PGDG), since Ubuntu’s default repos often have older versions.",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#connecting-with-python",
    "href": "core.html#connecting-with-python",
    "title": "fastasyncpg",
    "section": "Connecting with Python",
    "text": "Connecting with Python\nThe most popular Python libraries for PostgreSQL are psycopg2/psycopg3 (synchronous) and asyncpg (async). For async work, asyncpg is about 5x faster than psycopg3 and is the recommended choice.\nWe’ll use asyncpg for this wrapper — it’s the fastest Python PostgreSQL library for async code.\n\nimport os\n\n\nuser = os.environ['USER']; user\n\n'jhoward'\n\n\nasyncpg uses await for all database operations. The connect function returns a connection object, and fetchval is a convenience method that returns a single value from the first row:\n\nconn = await asyncpg.connect(user=user, database='postgres', host='127.0.0.1')\nawait conn.fetchval('SELECT version()')\n\n'PostgreSQL 18.1 (Homebrew) on aarch64-apple-darwin25.2.0, compiled by Apple clang version 17.0.0 (clang-1700.6.3.2), 64-bit'\n\n\nLet’s create a simple test table to explore basic operations:\n\nawait conn.execute('''DROP TABLE IF EXISTS users''')\nawait conn.execute('''CREATE TABLE users ( id SERIAL PRIMARY KEY, name TEXT NOT NULL, age INTEGER )''')\n\n'CREATE TABLE'\n\n\nGreat! Now let’s insert some data:\n\nawait conn.execute(\"INSERT INTO users (name, age) VALUES ($1, $2)\", 'Alice', 30)\n\n'INSERT 0 1'\n\n\nPostgreSQL uses $1, $2, etc. for parameterized queries, not ? like SQLite. This syntax allows you to reference the same parameter multiple times and makes the order explicit.\nfetch returns a list of Record objects. Each record supports dict-like access by column name or index:\n\nrs = await conn.fetch(\"SELECT * FROM users\")\nrs\n\n[&lt;Record id=1 name='Alice' age=30&gt;]\n\n\n\nr = rs[0]\ntype(rs),type(r)\n\n(list, asyncpg.protocol.record.Record)\n\n\nasyncpg.Record objects use dict-like access (r['name'] or r[0]), not attribute access. You can use dict2obj if you want the latter.\n\nro = dict2obj(dict(r))\nro.name\n\n'Alice'\n\n\nUnlike psycopg2, asyncpg doesn’t use traditional cursors. Instead, use async for record in conn.cursor(...) to iterate over results. However, cursors in asyncpg require an explicit transaction:\n\n# raises \"NoActiveSQLTransactionError: cursor cannot be created outside of a transaction\"\n# async for record in conn.cursor(\"SELECT * FROM users\"): print(record)\n\n\nasync with conn.transaction():\n    async for record in conn.cursor(\"SELECT * FROM users\"): print(record)\n\n&lt;Record id=1 name='Alice' age=30&gt;\n\n\nBy default, asyncpg operates in auto-commit mode — changes are applied immediately when not in an explicit transaction block. Regular queries (execute, fetch, etc.) don’t need transactions, but cursors do. This is a direct reflection of how PostgreSQL itself handles “portals” (the underlying mechanism for cursors).\nIn PostgreSQL, a portal is generally only valid for the duration of a transaction. If a transaction isn’t explicitly started, PostgreSQL runs each command in its own “one-shot” transaction. For a cursor to stay open so you can fetch multiple batches of rows, the transaction it belongs to must remain open.\nOther libraries (like psycopg2) often hide this by starting a transaction for you automatically when you create a cursor, whereas asyncpg chooses to be more “explicit” about the underlying database state.",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#chinook",
    "href": "core.html#chinook",
    "title": "fastasyncpg",
    "section": "Chinook",
    "text": "Chinook\nFor testing with a real dataset, we’ll use the Chinook sample database, which has tables for artists, albums, tracks, etc. The PostgreSQL version is available on GitHub:\ncurl -L -O https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_PostgreSql.sql\nNow we need to create a database and run that script. First, let’s create a database called chinook:\ncreatedb chinook\nThen we can load the SQL file into it:\npsql -d chinook -f Chinook_PostgreSql.sql\nAlways close connections when done — this releases the database connection back to PostgreSQL:\n\nawait conn.close()\n\nNow let’s connect to the Chinook database to work with more realistic data:\n\nconn = await asyncpg.connect(user=user, database='chinook', host='127.0.0.1')\nawait conn.fetchval(\"SELECT count(*) FROM artist\")\n\n275",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#metadata",
    "href": "core.html#metadata",
    "title": "fastasyncpg",
    "section": "Metadata",
    "text": "Metadata\nResults is a simple list subclass that renders as an HTML table in notebooks. It displays all rows with column headers, making query results easy to read:\n\nsource\n\nResults\n\ndef Results(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.\nsql is a quick helper to run SQL and return results as a list of records:\n\nfrom IPython.core.magic import register_cell_magic\n\n\n@register_cell_magic\nasync def sql(l,c): return Results(await conn.fetch(c))\n\n\nSELECT column_name, data_type, is_nullable\nFROM information_schema.columns\nWHERE table_name = 'artist'\n\n\n\n\ncolumn_name\ndata_type\nis_nullable\n\n\n\n\nartist_id\ninteger\nNO\n\n\nname\ncharacter varying\nYES\n\n\n\n\n\nIn PostgreSQL, every table belongs to a schema — think of it as a folder or namespace. The default schema is public. When you create a table without specifying a schema, it lands there. You can query schema information via information_schema views:\n\nSELECT column_name, data_type, is_nullable\nFROM information_schema.columns\nWHERE table_name = 'artist'\n\n\n\n\ncolumn_name\ndata_type\nis_nullable\n\n\n\n\nartist_id\ninteger\nNO\n\n\nname\ncharacter varying\nYES\n\n\n\n\n\nTo list all tables in the public schema, you can query the information_schema.tables view:\n\nawait conn.fetch(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\")\n\n[&lt;Record table_name='artist'&gt;,\n &lt;Record table_name='album'&gt;,\n &lt;Record table_name='employee'&gt;,\n &lt;Record table_name='customer'&gt;,\n &lt;Record table_name='invoice'&gt;,\n &lt;Record table_name='invoice_line'&gt;,\n &lt;Record table_name='track'&gt;,\n &lt;Record table_name='playlist'&gt;,\n &lt;Record table_name='playlist_track'&gt;,\n &lt;Record table_name='genre'&gt;,\n &lt;Record table_name='media_type'&gt;,\n &lt;Record table_name='cats'&gt;,\n &lt;Record table_name='cat'&gt;,\n &lt;Record table_name='dog'&gt;]\n\n\nTo customize how records behave, we need access to the underlying Record class:\nFRecord extends asyncpg’s Record with two conveniences: attribute access (r.name instead of r['name']) and HTML rendering for notebooks:\n\nsource\n\n\nFRecord\n\ndef FRecord(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nInitialize self. See help(type(self)) for accurate signature.\nWe can use record_class to auto-wrap with FRecord:\n\nawait conn.close()\n\nconn = await asyncpg.connect(user=user, database='chinook', host='127.0.0.1', record_class=FRecord)\n\ntable_names and view_names query PostgreSQL’s system catalogs to list tables and views in a schema. We use pg_class and pg_namespace rather than information_schema for better performance:\n\nsource\n\n\nview_names\n\ndef view_names(\n    conn, schema:str='public'\n):\n\nList of view names in schema\n\nsource\n\n\ntable_names\n\ndef table_names(\n    conn, schema:str='public'\n):\n\nList of table names in schema\n\nprint(' '.join(await table_names(conn)))\n\nartist album employee customer invoice invoice_line track playlist playlist_track genre media_type cats cat dog\n\n\n\nawait view_names(conn)\n\n[]\n\n\ncolumns_info returns a dict mapping column names to their PostgreSQL data types. It queries pg_attribute directly for efficiency:\n\nsource\n\n\ncolumns_info\n\ndef columns_info(\n    conn, table, schema:str='public'\n):\n\nDict mapping column names to data types for table\n\nlist(await columns_info(conn, 'artist'))\n\n['artist_id', 'name']\n\n\nWe’ll need to know each table’s primary key. PostgreSQL stores this in pg_index. The ::regclass cast is idiomatic PostgreSQL — it converts a table name string to its internal object ID, automatically handling schema resolution. The indisprimary flag identifies the primary key index.\n\nsource\n\n\npk_cols\n\ndef pk_cols(\n    conn, table\n):\n\nGet primary key column(s) for table\n\nawait pk_cols(conn, 'artist')\n\n['artist_id']\n\n\nDatabase wraps an asyncpg connection (or pool) and provides table/view metadata caching. It delegates unknown attributes to the underlying connection via __getattr__, so you can call db.fetch(...) directly. The t property returns a _TablesGetter for convenient table access.\n\nsource\n\n\nDatabase\n\ndef Database(\n    conn, refresh:bool=True\n):\n\nInitialize self. See help(type(self)) for accurate signature.\nTable represents a database table with metadata like columns and primary keys. The xtra method lets you set persistent row filters (useful for multi-tenancy). Tables stringify as quoted identifiers for safe SQL interpolation.\n\nsource\n\n\nTable\n\ndef Table(\n    db, name\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n_Getter is a base class for “magic accessor” objects that provide multiple ways to access items — by attribute (dt.artist), by index (dt['artist']), or by iteration (for t in dt). It implements __dir__ so tab-completion works in notebooks, __repr__ for nice display, __contains__ for in checks, and both __getattr__ and __getitem__ for flexible access.\n_TablesGetter specializes this for tables, reading from the database’s _tnames list. The db.t property returns one of these, giving you a clean API: db.t.artist instead of db.table('artist').\nconnect is our main entry point — it creates an asyncpg connection with FRecord as the default record class, sets up JSON codecs, and returns a Database wrapper with metadata already loaded:\n\nasync def connect(*args, **kwargs):\n    kwargs.setdefault('record_class', FRecord)\n    conn = await asyncpg.connect(*args, **kwargs)\n    res = Database(conn, refresh=False)\n    await res.refresh()\n    return res\n\n\nawait conn.close()\n\ndb = await connect(user=user, database='chinook', host='127.0.0.1'); str(db)\n\n'postgresql://jhoward@127.0.0.1:5432/chinook'\n\n\n\ndt = db.t\nartist = dt.artist\nartist\n\nTable \"artist\"\n\n\n\nartist.pks\n\n['artist_id']\n\n\n\ndt['album','artist']\n\n[Table \"album\", Table \"artist\"]\n\n\n\nfor tbl in dt:\n    if tbl.name[0]=='a': print(tbl)\n\n\"album\"\n\"artist\"\n\n\n\nassert 'artist' in dt\nassert artist in dt\nassert 'foo' not in dt\n\n\nartist.cols\n\n{'artist_id': 'integer', 'name': 'character varying(120)'}\n\n\n_Col represents a single column, with __str__ returning fully-qualified SQL (\"table\".\"column\"). _ColsGetter follows the same pattern as _TablesGetter — it’s a magic accessor that lets you write artist.c.name to get a column reference. The __call__ method returns all columns as _Col objects, useful for building queries programmatically.\n\nsource\n\n\nTable.c\n\ndef c(\n    \n):\n\n\nac = artist.c\nac\n\nartist_id, name\n\n\nColumns stringify in a format suitable for including in SQL statements.\n\nprint(f\"select {ac.name} ...\")\n\nselect \"artist\".\"name\" ...\n\n\nTables and views do the same.\n\nprint(f\"select {ac.Name} from {artist}\")\n\nselect \"artist\".\"Name\" from \"artist\"\n\n\n\nassert 'name' in ac\nassert ac.name in ac\nassert 'foo' not in ac",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#queries-and-views",
    "href": "core.html#queries-and-views",
    "title": "fastasyncpg",
    "section": "Queries and views",
    "text": "Queries and views\nWe will support ? in addition to the standard pgsql $1 form placeholders, by using sqlparse to parse the query.\nsqlparse returns a token list per statement, eg:\n\nts = sqlparse.parse(\"SELECT * FROM artist WHERE artist_id = ?\")[0].flatten()\nfor t in ts: print(repr(t.ttype), repr(t.value))\n\nToken.Keyword.DML 'SELECT'\nToken.Text.Whitespace ' '\nToken.Wildcard '*'\nToken.Text.Whitespace ' '\nToken.Keyword 'FROM'\nToken.Text.Whitespace ' '\nToken.Name 'artist'\nToken.Text.Whitespace ' '\nToken.Keyword 'WHERE'\nToken.Text.Whitespace ' '\nToken.Name 'artist_id'\nToken.Text.Whitespace ' '\nToken.Operator.Comparison '='\nToken.Text.Whitespace ' '\nToken.Name.Placeholder '?'\n\n\n\nsource\n\nconv_placeholders\n\ndef conv_placeholders(\n    sql, kwargs:VAR_KEYWORD\n):\n\nConvert ? and :name placeholders to PostgreSQL $n style\n\nprint(conv_placeholders(\"\"\"\n    SELECT * FROM a WHERE id = ?;\n    SELECT * FROM b WHERE name = ?\n\"\"\")[0])\n\n\n    SELECT * FROM a WHERE id = $1;\n    SELECT * FROM b WHERE name = $2\n\n\n\n\nconv_placeholders(\"SELECT * FROM artist WHERE artist_id = $1 AND name = ?\")\n\n('SELECT * FROM artist WHERE artist_id = $1 AND name = $1', [])\n\n\nWe get back values for the kwargs in the order they appear:\n\nconv_placeholders(\"SELECT * FROM artist WHERE artist_id = :id AND name = :name\", id=5, name='AC/DC')\n\n('SELECT * FROM artist WHERE artist_id = $1 AND name = $2', [5, 'AC/DC'])\n\n\nA repeated kw placeholder name results in a single arg result:\n\nconv_placeholders(\"SELECT * FROM a WHERE id = :uid OR creator = :uid\", uid=42)\n\n('SELECT * FROM a WHERE id = $1 OR creator = $1', [42])\n\n\nPlaceholder types can be mixed:\n\nconv_placeholders(\"SELECT * FROM a WHERE id = ? AND name = :name and foo=?\", name='test')\n\n('SELECT * FROM a WHERE id = $1 AND name = $3 and foo=$2', ['test'])\n\n\ndb.q is a convenience method that runs a SQL query and wraps results in a Results list for nice HTML rendering:\n\nsource\n\n\nDatabase.q\n\ndef q(\n    sql, args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\n\nacdc = await db.q(f\"select * from {artist} where {ac.name} like 'AC/%'\")\nacdc\n\n\n\n\nartist_id\nname\n\n\n\n\n1\nAC/DC\n\n\n\n\n\n\nawait db.q(f\"select * from {artist} where {ac.name}=$1\", 'AC/DC')\n\n\n\n\nartist_id\nname\n\n\n\n\n1\nAC/DC\n\n\n\n\n\n\nawait db.q(f\"select * from {artist} where {ac.name}=?\", 'AC/DC')\n\n\n\n\nartist_id\nname\n\n\n\n\n1\nAC/DC\n\n\n\n\n\n\nawait db.q(f\"select * from {artist} where {ac.name}=:name\", name='AC/DC')\n\n\n\n\nartist_id\nname\n\n\n\n\n1\nAC/DC",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#dataclasses",
    "href": "core.html#dataclasses",
    "title": "fastasyncpg",
    "section": "Dataclasses",
    "text": "Dataclasses\nPostgreSQL has many data types that map to Python equivalents. We’ll import the Python types we need:\nget_typ extracts the base PostgreSQL type (stripping size specifiers like (120)) and maps it to the corresponding Python type:\n\nsource\n\nget_typ\n\ndef get_typ(\n    pg_type\n):\n\nGet Python type for PostgreSQL type string\nThe pg_to_py dict maps PostgreSQL type names to Python types. This covers the most common types — numeric, string, temporal, JSON, network, and geometric:\nasyncpg doesn’t automatically decode JSON columns — we need to register custom codecs. The setup_json function configures both json and jsonb types to use Python’s json module:\n\nsource\n\n\nsetup_json\n\ndef setup_json(\n    conn\n):\n\nWe’ll re-define connect to use json now:\n\nsource\n\n\nconnect\n\ndef connect(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\n\nawait db.close()\n\n\ndb = await connect(user=user, database='chinook', host='127.0.0.1')\n\nWe’ll use Python’s dataclasses module to auto-generate typed classes from table schemas:\nWith the type mapping in place, we can auto-generate Python dataclasses from table schemas. The _get_flds helper extracts field definitions, and dataclass() creates a dataclass matching the table structure. We use flexiclass from fastcore to make the dataclass more flexible (allowing partial instantiation).\n\ndt = db.t\nartist = dt.artist\n\nArtist = artist.dataclass()\nart1_obj = Artist(**acdc[0])\nart1_obj\n\nArtist(artist_id=1, name='AC/DC')\n\n\n\nartist.cls\n\n__main__.Artist\n\n\nYou can get the definition of the dataclass using fastcore’s dataclass_src:\n\nsrc = dataclass_src(Artist)\nhl_md(src, 'python')\n\n@dataclass\nclass Artist:\n    artist_id: int | None = UNSET\n    name: str | None = UNSET\n\n\nall_dcs generates dataclasses for every table (and optionally views) in the database. This is useful for type-checking and IDE autocompletion:\n\nsource\n\n\nall_dcs\n\ndef all_dcs(\n    db, with_views:bool=False, store:bool=True, suf:str=''\n):\n\ndataclasses for all objects in db",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#get",
    "href": "core.html#get",
    "title": "fastasyncpg",
    "section": "get",
    "text": "get\nThe xtra method (defined earlier) lets you set persistent filters on a table. The _add_xtra helper injects these constraints into WHERE clauses. This is useful for multi-tenant apps or any situation where you want automatic row filtering — e.g., album.xtra(artist_id=1) ensures all subsequent queries only see albums by artist 1.\n__getitem__ provides dict-style access by primary key. It raises NotFoundError if the row doesn’t exist (or doesn’t match xtra constraints). If a dataclass has been generated for the table, results are automatically converted to that type.\n\nsource\n\nTable.__getitem__\n\ndef __getitem__(\n    pk\n):\n\nGet row by primary key, raising NotFoundError if missing\n\nsource\n\n\nNotFoundError\n\ndef NotFoundError(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nCommon base class for all non-exit exceptions.\n\na = await artist[1]\na\n\nArtist(artist_id=1, name='AC/DC')\n\n\n\na._db\n\n&lt;__main__.Database&gt;\n\n\n\nalbum = dt.album\nAlbum = album.dataclass()\n\nprint(\"Album 1:\", await album[1])\nprint(\"Artist ID of album 1:\", (await album[1]).artist_id)\n\nalbum.xtra(artist_id=1)\nprint(\"\\nWith xtra(artist_id=1):\")\nprint(\"Album 1:\", await album[1])  # Should work - album 1 is by artist 1\n\ntry: await album[2]  # Album 2 is by a different artist\nexcept NotFoundError as e: print('Error correctly raised:', type(e))\n\nAlbum 1: Album(album_id=1, title='For Those About To Rock We Salute You', artist_id=1)\nArtist ID of album 1: 1\n\nWith xtra(artist_id=1):\nAlbum 1: Album(album_id=1, title='For Those About To Rock We Salute You', artist_id=1)\nError correctly raised: &lt;class '__main__.NotFoundError'&gt;\n\n\nget is the “safe” version of __getitem__ — it returns None instead of raising an exception when a row isn’t found. This mirrors the pattern in fastlite and Python’s dict.get().\n\nsource\n\n\nTable.get\n\ndef get(\n    pk\n):\n\nGet row by primary key, or None if missing\n\nawait artist.get(1)\n\nArtist(artist_id=1, name='AC/DC')\n\n\n\nawait artist.get(99999)  # Should return None",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#callselect",
    "href": "core.html#callselect",
    "title": "fastasyncpg",
    "section": "call/select",
    "text": "call/select\nrows_where is the core query method. It builds SQL from its parameters, applies xtra constraints, and optionally converts results to the table’s dataclass. Unlike psycopg2/sqlite which use ? placeholders, PostgreSQL uses $1, $2 positional parameters.\n\nsource\n\nTable.rows_where\n\ndef rows_where(\n    where:NoneType=None, where_args:NoneType=None, order_by:NoneType=None, select:str='*', limit:NoneType=None,\n    offset:NoneType=None, as_cls:bool=True, debug:bool=False\n):\n\nIterate over rows matching where clause\n\nawait artist.rows_where(limit=3)\n\n[Artist(artist_id=1, name='AC/DC'),\n Artist(artist_id=2, name='Accept'),\n Artist(artist_id=3, name='Aerosmith')]\n\n\n\nalbum.xtra(artist_id=1)\nawait album.rows_where(limit=5)\n\n[Album(album_id=1, title='For Those About To Rock We Salute You', artist_id=1),\n Album(album_id=4, title='Let There Be Rock', artist_id=1)]\n\n\ncount is an async property that returns the number of rows in a table. It respects xtra constraints, so if you’ve set filters, only matching rows are counted:\n\nsource\n\n\nTable.count_where\n\ndef count_where(\n    where:NoneType=None, where_args:NoneType=None\n):\n\n\nsource\n\n\nTable.count\n\ndef count(\n    \n):\n\n\nalbum.xtra(artist_id=1)\nawait album.count\n\n2\n\n\n\nalbum.xtra()\nawait album.count\n\n347\n\n\nget_field extracts a value from either a dict-like object (using [k]) or a dataclass/object (using getattr). This lets us handle both Record and dataclass results uniformly:\n\nsource\n\n\nget_field\n\ndef get_field(\n    r, k\n):\n\npks_and_rows_where wraps rows_where but returns (pk, row) tuples — useful when you need to know which primary key each row has without inspecting the row itself.\n\nsource\n\n\nTable.pks_and_rows_where\n\ndef pks_and_rows_where(\n    kwargs:VAR_KEYWORD\n):\n\nLike rows_where but returns (pk, row) tuples\n\nawait artist.pks_and_rows_where(limit=3)\n\n[(1, Artist(artist_id=1, name='AC/DC')),\n (2, Artist(artist_id=2, name='Accept')),\n (3, Artist(artist_id=3, name='Aerosmith'))]\n\n\n__call__ makes tables callable, providing a convenient shorthand for queries. await artist(limit=3) is equivalent to await artist.rows_where(limit=3). The with_pk parameter switches to returning (pk, row) tuples.\n\nsource\n\n\nTable.__call__\n\ndef __call__(\n    where:NoneType=None, where_args:NoneType=None, order_by:NoneType=None, limit:NoneType=None, offset:NoneType=None,\n    select:str='*', with_pk:bool=False, as_cls:bool=True, debug:bool=False\n):\n\nQuery table rows\n\nawait artist(limit=3)\n\n[Artist(artist_id=1, name='AC/DC'),\n Artist(artist_id=2, name='Accept'),\n Artist(artist_id=3, name='Aerosmith')]\n\n\n\nawait artist(limit=3, with_pk=True)\n\n[(1, Artist(artist_id=1, name='AC/DC')),\n (2, Artist(artist_id=2, name='Accept')),\n (3, Artist(artist_id=3, name='Aerosmith'))]\n\n\n\nalbum.xtra(artist_id=1)\nawait album(limit=5)\n\n[Album(album_id=1, title='For Those About To Rock We Salute You', artist_id=1),\n Album(album_id=4, title='Let There Be Rock', artist_id=1)]\n\n\nselectone returns exactly one row matching the query, raising NotFoundError if none found or ValueError if multiple found. It passes limit=2 internally so it can detect the “not unique” case without fetching the entire table.\n\nsource\n\n\nTable.selectone\n\ndef selectone(\n    where:str | None=None, # SQL where fragment to use, for example `id &gt; ?`\n    where_args:Union=None, # Parameters to use with `where`; iterable for `id&gt;?`, or dict for `id&gt;:id`\n    select:str='*', # Comma-separated list of columns to select\n    as_cls:bool=True, # Convert returned dict to stored dataclass?\n    debug:bool=False\n)-&gt;list:\n\nShortcut for __call__ that returns exactly one item\n\nawait artist.selectone('Name=$1', ('AC/DC',), debug=True)\n\nSELECT * FROM \"artist\" WHERE Name=$1 LIMIT 2\n\n\nArtist(artist_id=1, name='AC/DC')\n\n\n\ntry: await artist.selectone('Name like $1', ('%a%',))\nexcept ValueError: pass\nelse: raise Exception(\"Failed to get non unique exception\")\n\n\ntry: await artist.selectone('Name=$1', ('i do not exist',))\nexcept NotFoundError: pass\nelse: raise Exception(\"Failed to get NotFoundError\")\n\ndb.item is for scalar queries — it returns a single field from a single row. Useful for things like SELECT count(*) or SELECT max(price).\n\nsource\n\n\nDatabase.item\n\ndef item(\n    sql, args:NoneType=None\n):\n\nExecute sql and return a single field from a single row\n\nawait db.item('select artist_id from artist where name=$1', ('AC/DC',))\n\n1",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#create_mod",
    "href": "core.html#create_mod",
    "title": "fastasyncpg",
    "section": "create_mod",
    "text": "create_mod\ncreate_mod generates a Python module file containing dataclass definitions for all tables in the database. This lets you import type-checked dataclasses directly rather than regenerating them each time. The generated file includes proper imports and uses UNSET defaults for flexible instantiation.\n\nsource\n\ncreate_mod\n\ndef create_mod(\n    db, mod_fn, with_views:bool=False, store:bool=True, suf:str=''\n):\n\nCreate module for dataclasses for db\n\ncreate_mod(db, 'db_dc')\n\nlink_dcs reconnects a database’s tables to dataclasses from a previously generated module. This is useful when you’ve imported dataclasses from a file created by create_mod and want the ORM to use them.\n\nsource\n\n\nDatabase.link_dcs\n\ndef link_dcs(\n    mod\n):\n\nSet the internal dataclass type links for tables using mod (created via create_mod)\n\nfrom db_dc import *\nawait dt.track[1]\n\nTrack(track_id=1, name='For Those About To Rock (We Salute You)', album_id=1, media_type_id=1, genre_id=1, composer='Angus Young, Malcolm Young, Brian Johnson', milliseconds=343719, bytes=11170334, unit_price=Decimal('0.99'))\n\n\nset_classes is a convenience method that links all table dataclasses from a namespace (typically globals()). It expects dataclass names to be title-cased versions of table names (e.g., Artist for table artist).\n\nsource\n\n\nDatabase.set_classes\n\ndef set_classes(\n    glb\n):\n\nAdd set all table dataclasses using types in namespace glb\n\ndb.t\n\nartist, album, employee, customer, invoice, invoice_line, track, playlist, playlist_track, genre, media_type, cats, cat, dog\n\n\nget_tables injects table objects into a namespace with pluralized names — so db.t.album becomes available as albums. Combined with set_classes, this gives you a clean API: await albums(limit=3) returns a list of Album dataclass instances.\n\nsource\n\n\nDatabase.get_tables\n\ndef get_tables(\n    glb\n):\n\nAdd objects for all table objects to namespace glb\n\ndb.set_classes(globals())\ndb.get_tables(globals())\n\nawait albums(limit=1)\n\n[Album(album_id=1, title='For Those About To Rock We Salute You', artist_id=1)]\n\n\n\nawait employees(limit=1)\n\n[Employee(employee_id=1, last_name='Adams', first_name='Andrew', title='General Manager', reports_to=None, birth_date=datetime.datetime(1962, 2, 18, 0, 0), hire_date=datetime.datetime(2002, 8, 14, 0, 0), address='11120 Jasper Ave NW', city='Edmonton', state='AB', country='Canada', postal_code='T5K 2N1', phone='+1 (780) 428-9482', fax='+1 (780) 428-3457', email='andrew@chinookcorp.com')]",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#insert",
    "href": "core.html#insert",
    "title": "fastasyncpg",
    "section": "insert",
    "text": "insert\nTo support both dataclasses and dicts as input, and to handle Enum values properly, we need these imports:\n_process_row converts a dataclass (or dict) to a plain dict, filtering out UNSET values and extracting .value from Enum fields. This lets you pass partially-filled dataclasses to insert/update.\ninsert adds a row to the table. It accepts either a dataclass/dict as record, keyword arguments, or both (kwargs override record fields). PostgreSQL’s RETURNING * clause lets us get the inserted row back in one query — including any auto-generated values like SERIAL primary keys. The xtra constraints are automatically merged in.\n\nsource\n\nTable.insert\n\ndef insert(\n    record:NoneType=None, kwargs:VAR_KEYWORD\n):\n\nInsert a row and return it\nFor DDL statements like CREATE TABLE, use execute rather than fetch/q. DDL statements don’t return rows — they return a status string like 'CREATE TABLE'. PostgreSQL uses SERIAL for auto-incrementing integers (instead of SQLite’s INTEGER PRIMARY KEY) and REAL instead of FLOAT.\n\nawait db.execute('''\nDROP TABLE IF EXISTS cat;\nCREATE TABLE cat (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    weight REAL,\n    uid INTEGER\n)''')\n\n'CREATE TABLE'\n\n\n_retr_tbl is a helper that refreshes the database metadata and returns the table object for a given name. This ensures you’re working with up-to-date schema information after creating or modifying tables.\ntable2glb is a convenience method that refreshes metadata, creates the dataclass, and injects both the table object (pluralized name) and the dataclass into a namespace. This is handy after creating a new table.\n\nsource\n\n\nDatabase.table2glb\n\ndef table2glb(\n    name, glb:NoneType=None\n):\n\nGet table by name, refreshing metadata and creating dataclass, adding to glb\n\nawait db.table2glb('cat')\n\n\ncats\n\nTable \"cat\"\n\n\n\nc = await cats.insert(name='meow', weight=6, uid=2)\nc\n\nCat(id=1, name='meow', weight=6.0, uid=2)\n\n\n\nawait cats()\n\n[Cat(id=1, name='meow', weight=6.0, uid=2)]\n\n\nWith xtra set, insert automatically includes those constraints. Here we set uid=1, so the inserted cat gets that value even though we didn’t pass it explicitly.\n\ncats.xtra(uid=1)\nc2 = await cats.insert(name='purr', weight=4)\nc2\n\nCat(id=2, name='purr', weight=4.0, uid=1)\n\n\n\nawait cats()\n\n[Cat(id=2, name='purr', weight=4.0, uid=1)]\n\n\nCalling xtra() with no arguments clears the filter by setting xtra_id = {}. Now queries return all rows again.\n\ncats.xtra()\n\nTable \"cat\"\n\n\n\nawait cats()  # Should now return all cats, not just uid=1\n\n[Cat(id=1, name='meow', weight=6.0, uid=2),\n Cat(id=2, name='purr', weight=4.0, uid=1)]",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#update",
    "href": "core.html#update",
    "title": "fastasyncpg",
    "section": "update",
    "text": "update\n\nc.name = \"moo\"\nc.uid = 1\nc\n\nCat(id=1, name='moo', weight=6.0, uid=1)\n\n\n_pk_where builds a WHERE clause for primary key matching, using PostgreSQL’s $1, $2 placeholders with an offset to account for preceding parameters in the query.\nupdate modifies an existing row by primary key. It builds an UPDATE ... SET ... WHERE pk = $n RETURNING * statement. Like insert, it respects xtra constraints — if you try to update a row that doesn’t match the xtra filter, you’ll get NotFoundError.\n\nsource\n\nTable.update\n\ndef update(\n    record:NoneType=None, pk_values:NoneType=None, kwargs:VAR_KEYWORD\n):\n\nUpdate a row and return it\n\nawait cats.update(c)\n\nCat(id=1, name='moo', weight=6.0, uid=1)\n\n\n\nawait cats()\n\n[Cat(id=2, name='purr', weight=4.0, uid=1),\n Cat(id=1, name='moo', weight=6.0, uid=1)]\n\n\n\ncats.xtra(uid=2)\nc.uid = 2\ntry: await cats.update(c)  # Should fail - c has id=1 which has uid=1, not uid=2\nexcept NotFoundError as e: print('Correctly blocked:', e)\n\nCorrectly blocked: cat[[1, 2]]\n\n\n\ncats.xtra()\n\nTable \"cat\"",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#delete",
    "href": "core.html#delete",
    "title": "fastasyncpg",
    "section": "delete",
    "text": "delete\ndelete removes a row by primary key, returning the deleted row (using RETURNING *). Like the other methods, it respects xtra constraints — attempting to delete a row that doesn’t match the filter raises NotFoundError.\n\nsource\n\nTable.delete\n\ndef delete(\n    pk_values\n):\n\nDelete row by primary key, returning the deleted row\nLet’s verify delete works — first check what cats we have:\n\nawait cats()\n\n[Cat(id=2, name='purr', weight=4.0, uid=1),\n Cat(id=1, name='moo', weight=6.0, uid=1)]\n\n\nDelete returns the deleted row, so you can see exactly what was removed:\n\nawait cats.delete(c.id)\n\nCat(id=1, name='moo', weight=6.0, uid=1)\n\n\n\nawait cats()\n\n[Cat(id=2, name='purr', weight=4.0, uid=1)]\n\n\nThe xtra filter also applies to deletes. If you try to delete a row that doesn’t match the constraint, you get NotFoundError:\n\ncats.xtra(uid=99)\ntry: await cats.delete(2)  # Should fail - cat 2 has uid=1, not uid=99\nexcept NotFoundError as e: print('Correctly blocked:', e)\n\nCorrectly blocked: cat[2]\n\n\n\ncats.xtra()\n\nTable \"cat\"\n\n\ndelete_where is the bulk version of delete — it removes all rows matching a WHERE clause (or all rows if none given), returning the deleted rows as a list. Like delete, it respects xtra constraints and uses RETURNING * to give back what was removed. This is useful for cleanup operations like removing all rows above a threshold.\n\nsource\n\n\nTable.delete_where\n\ndef delete_where(\n    where:NoneType=None, where_args:NoneType=None\n):\n\n\nawait cats.insert(name='Cat McCat Face', weight=9)\nawait cats.insert(name='Skitter', weight=2)\n\nCat(id=4, name='Skitter', weight=2.0, uid=None)\n\n\n\nawait cats.delete_where('weight &gt; $1', [3])\n\n[Cat(id=2, name='purr', weight=4.0, uid=1),\n Cat(id=3, name='Cat McCat Face', weight=9.0, uid=None)]\n\n\n\nawait cats()\n\n[Cat(id=4, name='Skitter', weight=2.0, uid=None)]",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#create",
    "href": "core.html#create",
    "title": "fastasyncpg",
    "section": "Create",
    "text": "Create\nTo create tables from Python classes, we need a reverse mapping from Python types to PostgreSQL types. We generate it from pg_to_py and override some entries for cleaner defaults (e.g., TEXT instead of character varying).\ncol_def builds a column definition for CREATE TABLE. If the column is the primary key and has type int, it becomes SERIAL PRIMARY KEY (PostgreSQL’s auto-incrementing integer). Otherwise it maps the Python type to PostgreSQL and adds NOT NULL if specified.\n\nsource\n\ncol_def\n\ndef col_def(\n    name, typ, pk, not_null\n):\n\nBuild column definition SQL for CREATE TABLE\n\ncol_def('id', int, 'id', None)  # 'id' is pk and int -&gt; SERIAL PRIMARY KEY\n\n'\"id\" SERIAL PRIMARY KEY'\n\n\n\ncol_def('name', str, 'id', {'name'})  # not pk, in not_null -&gt; TEXT NOT NULL\n\n'\"name\" TEXT NOT NULL'\n\n\n\ncol_def('age', int, 'id', None)  # not pk -&gt; INTEGER\n\n'\"age\" INTEGER'\n\n\ndb.create creates a table from a Python class (or dataclass). It extracts field names and types, builds column definitions, and executes the CREATE TABLE statement. The replace=True option drops any existing table first (with CASCADE to handle dependencies).\n\nsource\n\n\nDatabase.create\n\ndef create(\n    cls:NoneType=None, name:NoneType=None, pk:str='id', foreign_keys:NoneType=None, defaults:NoneType=None,\n    column_order:NoneType=None, not_null:NoneType=None, if_not_exists:bool=False, replace:bool=False\n):\n\nCreate table from cls\ndrop removes a table from the database. The cascade=True option also drops any dependent objects (like foreign key references from other tables).\n\nsource\n\n\nTable.drop\n\ndef drop(\n    cascade:bool=False\n):\n\nDrop this table\n\nawait db.t.dog.drop(cascade=True)\n\nNow let’s test create with a simple Dog class:\n\nclass Dog: id:int; name:str; age:int\n\ndogs = await db.create(Dog, replace=True)\ndogs.cols\n\n{'id': 'integer', 'name': 'text', 'age': 'integer'}\n\n\nThe auto-generated SERIAL primary key handles auto-increment automatically:\n\nd = await dogs.insert(name='Rex', age=5)\nd\n\nDog(id=1, name='Rex', age=5)\n\n\nForeign keys are specified as a dict mapping column names to (table, column) tuples:\n\nclass Toy: id:int; name:str; dog_id:int\n\ntoys = await db.create(Toy, replace=True, foreign_keys={'dog_id': ('dog', 'id')})\ntoys.cols\n\n{'id': 'integer', 'name': 'text', 'dog_id': 'integer'}\n\n\nThe foreign key constraint is enforced by PostgreSQL — inserting a toy with an invalid dog_id would raise an error:\n\nt = await toys.insert(name='Ball', dog_id=d.id)\nt\n\nToy(id=1, name='Ball', dog_id=1)",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#upsert",
    "href": "core.html#upsert",
    "title": "fastasyncpg",
    "section": "Upsert",
    "text": "Upsert\nupsert performs an “insert or update” operation using PostgreSQL’s ON CONFLICT ... DO UPDATE clause. If a row with the same primary key exists, it updates it; otherwise it inserts a new row. Like insert, it uses _prep_row for row processing and _exec_returning for result handling, and respects xtra constraints.\n\nsource\n\nTable.upsert\n\ndef upsert(\n    record:NoneType=None, kwargs:VAR_KEYWORD\n):\n\nInsert or update a row and return it\nLet’s test upsert — first check current state:\n\nawait dogs()\n\n[Dog(id=1, name='Rex', age=5)]\n\n\nUpdating an existing row — change Rex’s age from 5 to 6:\n\nd.age = 6\nawait dogs.upsert(d)\n\nDog(id=1, name='Rex', age=6)\n\n\n\nawait dogs()\n\n[Dog(id=1, name='Rex', age=6)]\n\n\nInserting a new row — upsert without an existing id creates a new record:\n\nawait dogs.upsert(name='Spot', age=3)\n\nDog(id=2, name='Spot', age=3)\n\n\n\nawait dogs()\n\n[Dog(id=1, name='Rex', age=6), Dog(id=2, name='Spot', age=3)]\n\n\nWith xtra set, upsert merges those constraints into the row:\n\ndogs.xtra(age=6)\nd.name = 'Rexy'\nawait dogs.upsert(d)  # Should set age=6 from xtra\n\nDog(id=1, name='Rexy', age=6)",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "core.html#connection-pool",
    "href": "core.html#connection-pool",
    "title": "fastasyncpg",
    "section": "Connection pool",
    "text": "Connection pool\nFor production use, you’ll want a connection pool instead of a single connection. asyncpg.Pool has the same query methods (fetch, execute, etc.) as a connection, so our Database wrapper works with both. The key difference is that JSON codecs must be registered via the init callback (which runs on each new connection in the pool).\n\nsource\n\ncreate_pool\n\ndef create_pool(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\n\nawait db.close()\n\nLet’s test that the pool works the same as a single connection:\n\ndb = await create_pool(user=user, database='chinook', host='127.0.0.1')\nawait db.t.artist(limit=3)\n\n[&lt;FRecord artist_id=1 name='AC/DC'&gt;,\n &lt;FRecord artist_id=2 name='Accept'&gt;,\n &lt;FRecord artist_id=3 name='Aerosmith'&gt;]\n\n\n\nstr(db)\n\n'postgresql://jhoward@127.0.0.1:5432/chinook'",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fastasyncpg",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "fastasyncpg",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall fastasyncpg in Development mode\n# make sure fastasyncpg package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to fastasyncpg\n$ nbdev_prepare",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "fastasyncpg",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/AnswerDotAI/fastasyncpg.git\nor from conda\n$ conda install -c AnswerDotAI fastasyncpg\nor from pypi\n$ pip install fastasyncpg\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "fastasyncpg"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "fastasyncpg",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "fastasyncpg"
    ]
  }
]